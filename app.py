# -*- coding: utf-8 -*-
"""FalconAI-Depploy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UvBor3DWPFh9WeLRaFvKwoywtNwrsHjc
"""

import os
import re
import torch
import torchaudio
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from pydub import AudioSegment
from flask import Flask, request, jsonify
from dotenv import load_dotenv
from transformers import (
    WhisperProcessor,
    WhisperForConditionalGeneration,
    MarianMTModel,
    MarianTokenizer,
    AutoTokenizer,
    AutoModelForSeq2SeqLM
)
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Pinecone as PineconeVectorStore
from pinecone import Pinecone, ServerlessSpec
from langdetect import detect

# -------------------- Load Environment Variables --------------------
load_dotenv()  # טוען משתני סביבה מקובץ .env (אם קיים)
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
if not PINECONE_API_KEY:
    raise ValueError("❌ Missing PINECONE_API_KEY in environment variables.")

# -------------------- Supported languages for translation --------------------
supported_languages = {
    "en": "english",
    "fr": "french",
    "es": "spanish",
    "ru": "russian",
    # Add more if needed
}

def detect_language_of_text(text):
    """
    Detects the text language using langdetect.
    Returns 'en', 'fr', 'es', or 'ru' if detected. Defaults to 'en' otherwise.
    """
    try:
        lang_code = detect(text)
    except:
        lang_code = "en"

    if lang_code not in supported_languages:
        return "en"
    return lang_code

# ---------- Whisper + Translation Functions ----------
def convert_audio_to_pcm(wav_file):
    """
    Converts any WAV to PCM 16kHz mono.
    Returns the path of the converted file.
    """
    audio = AudioSegment.from_file(wav_file)
    audio = audio.set_channels(1)
    audio = audio.set_frame_rate(16000)
    converted_file = wav_file.replace(".wav", "_converted.wav")
    audio.export(converted_file, format="wav")
    return converted_file

def load_whisper_model():
    model_name = "openai/whisper-small"
    processor = WhisperProcessor.from_pretrained(model_name)
    model = WhisperForConditionalGeneration.from_pretrained(model_name)
    return processor, model

def transcribe_audio_whisper(wav_file):
    processor, model = load_whisper_model()
    speech_array, sampling_rate = torchaudio.load(wav_file)
    if sampling_rate != 16000:
        transform = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)
        speech_array = transform(speech_array)

    input_features = processor(speech_array.squeeze(0), sampling_rate=16000, return_tensors="pt").input_features

    with torch.no_grad():
        predicted_ids = model.generate(
            input_features,
            task="transcribe",
            return_dict_in_generate=True,
            output_scores=True
        )
        transcription = processor.batch_decode(predicted_ids.sequences, skip_special_tokens=True)[0]

    # Detect language (from the token Whisper adds)
    detected_lang_id = predicted_ids.sequences[0][1].item()
    detected_lang = processor.tokenizer.convert_ids_to_tokens([detected_lang_id])[0]
    detected_lang = detected_lang.replace("<|", "").replace("|>", "")
    if detected_lang not in supported_languages:
        detected_lang = "en"

    return transcription, detected_lang

def load_translation_model(source_lang, target_lang):
    model_name = f"Helsinki-NLP/opus-mt-{source_lang}-{target_lang}"
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name)
    return tokenizer, model

def translate_text(text, source_lang, target_lang):
    """
    Translates text from source_lang to target_lang using Marian.
    If source_lang == target_lang, returns the text as is.
    """
    if source_lang == target_lang:
        return text
    tokenizer, model = load_translation_model(source_lang, target_lang)
    tokenized_text = tokenizer([text], return_tensors="pt", padding=True)
    with torch.no_grad():
        translated = model.generate(**tokenized_text)
    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)
    return translated_text

# ---------- Pinecone init ----------
INDEX_NAME = "organizationdata2"
pc = Pinecone(api_key=PINECONE_API_KEY, spec=ServerlessSpec(cloud="aws", region="us-east-1"))
existing_indexes = pc.list_indexes() or []
if INDEX_NAME not in existing_indexes:
    print(f"⚠️ Index '{INDEX_NAME}' not found. Creating a new one...")
    pc.create_index(
        name=INDEX_NAME,
        dimension=768,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
    print("✅ Index created successfully!")
else:
    print(f"✅ Index '{INDEX_NAME}' already exists.")
index = pc.Index(INDEX_NAME)

# ---------- Load data (TSLA.csv) ----------
file_path = "TSLA.csv"  # הנח שהקובץ נמצא באותה תיקייה
if not os.path.exists(file_path):
    raise FileNotFoundError(f"❌ The file '{file_path}' was not found.")

df = pd.read_csv(file_path, encoding="utf-8", encoding_errors="ignore")
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df['Close'] = pd.to_numeric(df['Close'], errors='coerce')
df['Open']  = pd.to_numeric(df['Open'], errors='coerce')
df = df.dropna(subset=['Date']).sort_values(by='Date').reset_index(drop=True)
print("✅ CSV file loaded successfully! Number of records:", len(df))

embedding_model = "sentence-transformers/all-mpnet-base-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model)

# ---------- LSTM Model Training ----------
class LSTMModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=50, num_layers=2):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        out, (hn, cn) = self.lstm(x)
        out = self.fc(out)
        return out

prices = df['Close'].values.astype(float)
x_data = prices[:-1]
y_data = prices[1:]

x_train = torch.tensor(x_data, dtype=torch.float32).unsqueeze(-1).unsqueeze(0)
y_train = torch.tensor(y_data, dtype=torch.float32).unsqueeze(-1).unsqueeze(0)

lstm_model = LSTMModel(input_size=1, hidden_size=50, num_layers=2)
criterion = nn.MSELoss()
optimizer = optim.Adam(lstm_model.parameters(), lr=0.01)

print("⏳ Training LSTM model...")
for epoch in range(300):  # אפשר לשנות ל-500 אם תרצה
    lstm_model.train()
    optimizer.zero_grad()
    output = lstm_model(x_train)
    loss = criterion(output, y_train)
    loss.backward()
    optimizer.step()
    if epoch % 100 == 0:
        print(f"Epoch {epoch}, Loss={loss.item():.4f}")
print("✅ LSTM Model trained.")

# ---------- T5 Model ----------
T5_MODEL = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(T5_MODEL)
t5_model = AutoModelForSeq2SeqLM.from_pretrained(T5_MODEL)

# ---------- Helper functions ----------
def fuzzy_is_lowest(question):
    variants = ["lowest", "lowse", "lowes", "lwest", "lowest price"]
    q_lower = question.lower()
    return any(v in q_lower for v in variants)

def fuzzy_is_highest(question):
    variants = ["highest", "higest", "hghest", "highst", "highest price"]
    q_lower = question.lower()
    return any(v in q_lower for v in variants)

def extract_days_ahead(question):
    q_lower = question.lower()
    match = re.search(r'\bin\s+(\d+)\s+days?\b', q_lower)
    if match:
        return int(match.group(1))
    if "month" in q_lower:
        return 30
    if "week" in q_lower:
        return 7
    mm = re.search(r'(\d+)\s+months', q_lower)
    if mm:
        return 30 * int(mm.group(1))
    return 30

def query_lstm_forecast(question):
    days_ahead = extract_days_ahead(question)
    recent_price = df['Close'].values[-1]
    current_input = torch.tensor([[[recent_price]]], dtype=torch.float32)
    predictions = []
    lstm_model.eval()
    with torch.no_grad():
        for i in range(days_ahead):
            pred = lstm_model(current_input)
            next_price = pred[0, -1, 0].item()
            predictions.append(next_price)
            current_input = torch.tensor([[[next_price]]], dtype=torch.float32)

    final_prediction = predictions[-1]
    return f"Based on the LSTM model, the predicted price in {days_ahead} days is approximately {final_prediction:.2f} USD."

def query_numeric_data(question):
    year_match = re.findall(r'\b(20[0-3]\d|19\d{2})\b', question)
    is_lowest  = fuzzy_is_lowest(question)
    is_highest = fuzzy_is_highest(question)

    if len(year_match) == 1:
        year = int(year_match[0])
        data_year = df[df['Date'].dt.year == year]
        if data_year.empty:
            return f"No data for {year}."

        if is_lowest:
            val = data_year['Close'].min()
            return f"The lowest closing price in {year} was {val:.2f}"
        elif is_highest:
            val = data_year['Close'].max()
            return f"The highest closing price in {year} was {val:.2f}"
        else:
            avg_val = data_year['Close'].mean()
            return f"For {year}, the average closing price was {avg_val:.2f}."

    if len(year_match) == 2:
        start_year = int(year_match[0])
        end_year   = int(year_match[1])
        if start_year > end_year:
            start_year, end_year = end_year, start_year

        data_range = df[(df['Date'].dt.year >= start_year) & (df['Date'].dt.year <= end_year)]
        if data_range.empty:
            return f"No data for {start_year}-{end_year}."

        if is_lowest:
            val = data_range['Close'].min()
            return f"The lowest closing price between {start_year} and {end_year} was {val:.2f}"
        elif is_highest:
            val = data_range['Close'].max()
            return f"The highest closing price between {start_year} and {end_year} was {val:.2f}"

        avg_val = data_range['Close'].mean()
        return f"The average closing price between {start_year} and {end_year} was {avg_val:.2f}."

    return None

def query_t5(question):
    inputs = tokenizer(question, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        output = t5_model.generate(**inputs, max_length=200)
    return tokenizer.decode(output[0], skip_special_tokens=True)

def analyze_question(question):
    q_lower = question.lower()
    if any(word in q_lower for word in ["predict", "forecast", "future", "תחזית"]):
        return "LSTM"
    elif re.search(r'\d{4}', q_lower) or any(word in q_lower for word in ["open", "close", "price", "high", "low"]):
        return "Pandas"
    else:
        return "T5"

def handle_single_question(question_in_english):
    """
    Handles a single question in English, returns answer in English.
    """
    normalized_q = question_in_english.strip().lower()
    if normalized_q in ["thanks", "thank you"]:
        return "You're welcome!"

    response_type = analyze_question(question_in_english)
    if response_type == "Pandas":
        ans = query_numeric_data(question_in_english)
        if ans is None:
            ans = query_t5(question_in_english)
    elif response_type == "LSTM":
        ans = query_lstm_forecast(question_in_english)
    else:
        ans = query_t5(question_in_english)
    return ans

# ---------- Flask App ----------
app = Flask(__name__)

@app.route("/")
def home():
    return "Hello! This is your AI endpoint. Send a POST request to /predict with JSON: {\"text\": \"your question\"}."

@app.route("/predict", methods=["POST"])
def predict():
    """
    Endpoint שמקבל JSON עם מפתח 'text':
    {
      "text": "השאלה של המשתמש"
    }
    1. נזהה שפה
    2. נתרגם לאנגלית
    3. נפעיל את המודל
    4. נתרגם חזרה
    5. נחזיר תשובה כ-JSON
    """
    data = request.get_json()
    if not data or "text" not in data:
        return jsonify({"error": "No 'text' provided"}), 400

    user_text = data["text"]
    detected_lang = detect_language_of_text(user_text)

    # תרגום לשפה האנגלית
    question_in_english = translate_text(user_text, detected_lang, "en")
    # הפעלת המודל
    answer_in_english = handle_single_question(question_in_english)
    # תרגום תשובה חזרה לשפת המקור
    final_answer = translate_text(answer_in_english, "en", detected_lang)

    return jsonify({"answer": final_answer})

# (אופציונלי) Endpoint לדוגמה לתמלול קובץ WAV
@app.route("/transcribe", methods=["POST"])
def transcribe_endpoint():
    """
    דוגמה לאיך אפשר לקבל קובץ אודיו בפורמט WAV ולהחזיר תמלול.
    שולחים multipart/form-data עם file=...
    """
    if "file" not in request.files:
        return jsonify({"error": "No file uploaded"}), 400

    file = request.files["file"]
    filename = "temp.wav"
    file.save(filename)

    # Convert to PCM 16kHz
    converted_wav = convert_audio_to_pcm(filename)

    try:
        recognized_text, detected_lang = transcribe_audio_whisper(converted_wav)
    except Exception as e:
        return jsonify({"error": f"Error in speech-to-text: {e}"}), 500

    # אפשר להחזיר רק את הטקסט, או גם מידע על השפה
    return jsonify({
        "transcription": recognized_text,
        "detected_language": detected_lang
    })

if __name__ == "__main__":
    # להרצה מקומית (בענן משתמשים בפקודה דרך Procfile/Gunicorn)
    app.run(host="0.0.0.0", port=5000)